{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle - Leaf competition\n",
    "### Topic: ensembling basic classifiers with a VotingClassifier\n",
    "* Models: KNN, DecisionTree, SVC\n",
    "\n",
    "* Ensambling: VotingClassifier\n",
    "\n",
    "* Tuning: GridSearchCV\n",
    "\n",
    "* CV: inner/outer: StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train_id = train.pop('id')\n",
    "train_y = train.pop('species')\n",
    "test_id = test.pop('id')\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(train_y)\n",
    "n_samples, n_features = train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The innner CV defines **random splits used for hyperparameter tuning cross validation**. We will use different random splits (though perform on the same data) to estimate the generalization error. This is to reduce the bias arising from the fact that the inner CV splits where used to select the hyperparameters and so the out of fold error rates will be too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inner CV\n",
    "skf = StratifiedKFold(5, shuffle=True) # 10 obs. per class, select 2 for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set **the three base classifiers and their voting ensemble**. We put the ensemble into a pipepline. There are no more steps in the pipeline at this point. We can add more steps while developing the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier() #max_depth=4)\n",
    "clf2 = KNeighborsClassifier() #n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf') #, probability=True)\n",
    "estimators=[('dt', clf1), ('knn', clf2),('svc', clf3)]\n",
    "n_estimators = len(estimators)\n",
    "eclf = VotingClassifier(estimators)#, n_jobs=-1) #voting='soft', weights=[2, 1, 2])\n",
    "pipeline = Pipeline([('vc', eclf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.1. Simple fitting without hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fitting (ensemble)\n",
    "pipeline.fit(train, y)\n",
    "# validation (ensemble)\n",
    "scores = cross_val_score(pipeline, train, y, cv=skf, n_jobs=-1)\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), 2*scores.std(ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.676 (+/- 0.101) [Decision Tree]\n",
      "Accuracy: 0.858 (+/- 0.049) [KNN]\n",
      "Accuracy: 0.797 (+/- 0.021) [SVC-RBF]\n",
      "Accuracy: 0.852 (+/- 0.061) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "# CV score of all classifiers and the ensemble (before GridSearch tuning)\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Decision Tree', 'KNN', 'SVC-RBF', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, train, y, cv=skf, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f) [%s]\" % (scores.mean(), 2*scores.std(ddof=1), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2. Tuning hyperparameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'vc__svc__gamma': 0.05208333333333333, 'vc__dt__max_depth': None, 'vc__knn__n_jobs': -1, 'vc__knn__n_neighbors': 2} with a score of 0.870\n"
     ]
    }
   ],
   "source": [
    "g = 1/n_features\n",
    "parameters = {'vc__dt__max_depth':[5,10,None],\n",
    "              'vc__knn__n_neighbors':[2,3,5],\n",
    "              'vc__knn__n_jobs':[-1],\n",
    "              'vc__svc__gamma':[10*g, g, g/10]}\n",
    "gs = GridSearchCV(pipeline, parameters, cv=skf, n_jobs=-1)\n",
    "gs.fit(train, y)\n",
    "print(\"The best parameters are %s with a score of %0.3f\" % (gs.best_params_, gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.691 (+/- 0.034) [DecisionTreeClassifier]\n",
      "Accuracy: 0.873 (+/- 0.040) [KNeighborsClassifier]\n",
      "Accuracy: 0.785 (+/- 0.055) [SVC]\n"
     ]
    }
   ],
   "source": [
    "# Explore the individual estimators\n",
    "gs.best_estimator_\n",
    "gs.best_estimator_.named_steps['vc']\n",
    "gs.best_estimator_.named_steps['vc'].estimators_\n",
    "gs.best_estimator_.named_steps['vc'].estimators_[0]\n",
    "# CV score for individual classifiers (after GridSearch tuning)\n",
    "for i in np.arange(n_estimators):\n",
    "    scores = cross_val_score(gs.best_estimator_.named_steps['vc'].estimators_[i],\n",
    "                             train, y, cv=skf, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f) [%s]\" % (scores.mean(), 2*scores.std(ddof=1),\n",
    "                            estimators[i][1].__class__.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertuning of the ensemble increased the performance of Decision Tree and KNN and decreased it for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874 (+/- 0.052)\n"
     ]
    }
   ],
   "source": [
    "# validation (ensemble after GridSearch tuning) (remember to use the .best_estimator_)\n",
    "scores = cross_val_score(gs.best_estimator_, train, y, cv=skf, n_jobs=-1)\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), 2*scores.std(ddof=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting accuracy of the ensemble is higher than for any individual model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
